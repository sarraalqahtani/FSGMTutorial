# FGSMTutorial
In this tutorial, you will learn how to use a Python library called CleverHans from OpenAI to compute adversarial examples that cause the classification accuracy of your MNIST model to significantly drop following the FGSM attack method as introduced in this paper:https://arxiv.org/abs/1412.6572

**FSGM**
We begin with deriving a simple way of constructing an adversarial example around an input (x, y).
Supppose we denote our neural network by a function f : X → {0, . . . , 9}.
Suppose we want to find a small perturbation ∆ of x such that the neural network f assigns a label
different from y to x+∆. To find such a ∆, we want to increase the cross-entropy loss of the network f
at (x, y); in other words, we want to take a small step ∆ along which the cross-entropy loss increases,
thus causing a misclassification. We can write this as a gradient ascent update, and to ensure that we
only take a small step, we can just use the sign of each coordinate of the gradient. The final algorithm
is this:
x˜ = x + ε · sign(∇L(f, x, y)),
where L is the cross-entropy loss.

First, implement the Fast Gradient Sign Method (FGSM) for the neural network given to you in the last tutorial. Then,
evaluate and report the accuracy of the neural network on adversarial examples. This is computed
as follows – 
for each test example x(i),generate an adversarial example ˜x(i)
for ε= 0.1. 
The neural network is correct if it predicts y(i) on ˜x(i) and wrong otherwise.

*Tutorial Steps*
1) Import dependent libraries (Tensorflow 2.0 required)
```python
from cleverhans.attacks import FastGradientMethod
from cleverhans.utils_keras import KerasModelWrapper 
from cleverhans.dataset import MNIST
```
2) load your pre-trained model from the last tutorial (lets call it MNIST_model)
3) evaluate the accuracy of MNIST_model and print results
4) initialize the Fast Gradient Sign Method (FGSM) attack object and graph using:
```python
fgsm = FastGradientMethod(wrap, sess=sess)
  fgsm_params = {'eps': 0.3,
                 'clip_min': 0.,
                 'clip_max': 1.}
  adv_x = fgsm.generate(x, ** fgsm_params)
  Consider the attack to be constant
  adv_x = tf.stop_gradient(adv_x)
  preds_adv = model(adv_x)
```
**New Method and Pseudocode (Optional)**
Next, design your own method to find adversarial examples. Your algorithm should take as input a
labeled example (x, y) and a perturbation amount ε, and output an adversarial example ˜x such that
the infinity norm of the difference between x and ˜x, is at most ε.
Describe your algorithm, and write down clear pseudocode for it.

**Experimental results**
A (clearly labeled) table or graph of results showing the accuracy of the given neural network on adversarial examples generated by your algorithm vs. the fast gradient sign method on the given MNIST as a function of ε. Report the accuracy on adversarial examples for ε= 0.05, 0.1, 0.15, 0.2. For
any strategy with randomness, you should do several experiments and give error bars – give all relevant
details.
The pseudocode and experimental details must contain all information needed to reproduce the results.

**Critical evaluation**
Is your method a clear improvement over FSGM? Is there further scope for improvement? What would you like to try next?
